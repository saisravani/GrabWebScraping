Quality Checks:
- eliminated duplicate rows in the dataframe
- Handled missing values, and inserted null or appropriate message in case of missing vallues using pandas
-verified the data types and data of each column and made changes accordingly

Approach
Objective: To develop a web scraper for Grab Food Delivery in Singapore to extract the restaurant details and store them in ndjson gzip

Tools and Technologies:
Programming Language: Python
Libraries: Requests, Selenium, BeautifulSoup, Pandas, time, jsonlines, gzip
Output Format: Newline-delimited JSON (NDJSON)
Compression: gzip


Steps:
Initial Setup:
Install necessary libraries.
Set up the environment for web scraping.


Data Extraction:
Send HTTP requests to the target website.
Parse HTML content using BeautifulSoup.
Extract necessary data points and store them in a structured format.


Data Storage:
Convert extracted data into a Pandas DataFrame.
Save the DataFrame into NDJSON format.
Compress the NDJSON file using gzip.



Quality Checks:
Check for missing values and duplicates.
Validate data types and data.


Final Output:
Ensure data is correctly formatted and stored.
Provide compressed data file for efficient storage and transfer.



The problem faced during the implementation

Problem 1: HTTP Request Issues
Description: Initially, faced issues with HTTP requests, such as receiving incorrect responses or getting blocked by the website.

Solution/Approach:
1. tried changing the location to Singapore, using VPN, but it didn't make any difference.
Later, I realized, it was blocking me if i make multiple requests to the website continously.
so, I just need to wait for some time before making another request after getting blocked.
2. User-Agent Headers: Implemented custom headers to mimic a real browser, which helped in avoiding blocks and receiving correct responses.

Problem 2: Dealing with dynamically loaded content
Description:
1. Initially, when the data was loaded, it was always returning just 32 restaurants.
2. while trying to extract the src from the image tag, there was no src attribute in the tag which was retrieved by BeautifulSoup,
but src was present in the browser when the image was inspected using browser tools.

Solution/Approach
1. Used Selenium to scroll through the webpage and to retrieve dynamic attributes of the tags.
Selenium can handle dynamic web content generated by JavaScript, which is a limitation for BeautifulSoup.

Improvements or Optimizations:
1. error handling, multithreading and logging could be incorporated to improve performance and robustness
2. dynamically loaded content could have been handled more effectively.